"""
–ú–æ–¥—É–ª—å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è.
–û–±—É—á–∞–µ—Ç –º–æ–¥–µ–ª–∏ –Ω–∞ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –∏—Ö –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è.
"""

import pandas as pd
import numpy as np
from pathlib import Path
from typing import Dict, Any, List, Tuple
import pickle
import json
from datetime import datetime
from sklearn.model_selection import train_test_split, TimeSeriesSplit
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import sys
sys.path.append(str(Path(__file__).parent.parent))
from core.database import Database


class ModelTrainer:
    """
    –ö–ª–∞—Å—Å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–∞ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö.
    """
    
    def __init__(self, config, logger):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ç—Ä–µ–Ω–µ—Ä–∞ –º–æ–¥–µ–ª–µ–π.
        
        Args:
            config: –û–±—ä–µ–∫—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ —Å–∏—Å—Ç–µ–º—ã
            logger: –û–±—ä–µ–∫—Ç –ª–æ–≥–≥–µ—Ä–∞
        """
        self.config = config
        self.logger = logger
        
        # –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è –º–æ–¥–µ–ª–µ–π
        self.models_dir = config.base_path / "models"
        self.models_dir.mkdir(parents=True, exist_ok=True)
        
        # –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö
        db_path = config.base_path / "data" / "market_data.db"
        self.database = Database(db_path, logger)
        
        # CSV –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è
        self.csv_dir = config.base_path / "data" / "csv"
        
        # –î–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏
        self.available_models = {
            'linear': LinearRegression(),
            'ridge': Ridge(alpha=1.0),
            'lasso': Lasso(alpha=1.0),
            'random_forest': RandomForestRegressor(n_estimators=100, random_state=42),
            'gradient_boosting': GradientBoostingRegressor(n_estimators=100, random_state=42)
        }
        
        self.logger.info("ModelTrainer –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
    
    def load_training_data(self, ticker: str, from_db: bool = True) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """
        –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è (–∫–æ—Ç–∏—Ä–æ–≤–∫–∏ + –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã).
        
        Args:
            ticker (str): –¢–∏–∫–µ—Ä –∞–∫—Ü–∏–∏
            from_db (bool): –ó–∞–≥—Ä—É–∂–∞—Ç—å –∏–∑ –ë–î (True) –∏–ª–∏ CSV (False)
        
        Returns:
            Tuple[pd.DataFrame, pd.DataFrame]: (–∫–æ—Ç–∏—Ä–æ–≤–∫–∏, –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã)
        """
        self.logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è: {ticker}")
        
        if from_db:
            # –ò–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
            quotes = self.database.load_quotes(ticker)
            indicators = self.database.load_indicators(ticker)
        else:
            # –ò–∑ CSV
            quotes_file = self.csv_dir / ticker / f"{ticker}.csv"
            indicators_file = self.csv_dir / ticker / f"{ticker}_indicators.csv"
            
            quotes = pd.read_csv(quotes_file, parse_dates=['date']) if quotes_file.exists() else pd.DataFrame()
            indicators = pd.read_csv(indicators_file, parse_dates=['date'], index_col='date') if indicators_file.exists() else pd.DataFrame()
        
        self.logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(quotes)} –∫–æ—Ç–∏—Ä–æ–≤–æ–∫ –∏ {len(indicators)} –∑–∞–ø–∏—Å–µ–π –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤")
        return quotes, indicators
    
    def prepare_features(self, quotes: pd.DataFrame, indicators: pd.DataFrame, 
                        target_horizon: int = 1) -> Tuple[pd.DataFrame, pd.Series]:
        """
        –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –¥–ª—è –æ–±—É—á–µ–Ω–∏—è.
        
        Args:
            quotes (pd.DataFrame): –ö–æ—Ç–∏—Ä–æ–≤–∫–∏
            indicators (pd.DataFrame): –ò–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
            target_horizon (int): –ì–æ—Ä–∏–∑–æ–Ω—Ç –ø—Ä–æ–≥–Ω–æ–∑–∞ –≤ –¥–Ω—è—Ö
        
        Returns:
            Tuple[pd.DataFrame, pd.Series]: (–ø—Ä–∏–∑–Ω–∞–∫–∏ X, —Ü–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è y)
        """
        self.logger.info(f"–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (horizon={target_horizon})")
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –∫–æ—Ç–∏—Ä–æ–≤–∫–∏ –∏ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
        if 'date' in quotes.columns:
            quotes = quotes.set_index('date')
        
        # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ
        data = quotes.join(indicators, how='left')
        
        # –£–¥–∞–ª—è–µ–º NaN
        data = data.dropna()
        
        if len(data) < target_horizon + 10:
            raise ValueError("–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
        
        # –°–æ–∑–¥–∞—ë–º —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é (—Ü–µ–Ω–∞ —á–µ—Ä–µ–∑ N –¥–Ω–µ–π)
        data['target'] = data['close'].shift(-target_horizon)
        
        # –£–¥–∞–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ N —Å—Ç—Ä–æ–∫ (–≥–¥–µ –Ω–µ—Ç —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π)
        data = data[:-target_horizon]
        
        # –ü—Ä–∏–∑–Ω–∞–∫–∏
        feature_columns = [col for col in data.columns if col not in ['target', 'date']]
        X = data[feature_columns]
        y = data['target']
        
        self.logger.info(f"–ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ {len(X)} –ø—Ä–∏–º–µ—Ä–æ–≤ —Å {len(feature_columns)} –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏")
        return X, y
    
    def train_model(self, X: pd.DataFrame, y: pd.Series, model_type: str = 'linear',
                   test_size: float = 0.2) -> Dict[str, Any]:
        """
        –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏.
        
        Args:
            X (pd.DataFrame): –ü—Ä–∏–∑–Ω–∞–∫–∏
            y (pd.Series): –¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è
            model_type (str): –¢–∏–ø –º–æ–¥–µ–ª–∏
            test_size (float): –†–∞–∑–º–µ—Ä —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏
        
        Returns:
            Dict[str, Any]: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—É—á–µ–Ω–∏—è
        """
        self.logger.info(f"–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏: {model_type}")
        
        if model_type not in self.available_models:
            raise ValueError(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –º–æ–¥–µ–ª—å: {model_type}")
        
        # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/test (—Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –ø–æ—Ä—è–¥–∫–∞ –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤)
        split_idx = int(len(X) * (1 - test_size))
        X_train, X_test = X[:split_idx], X[split_idx:]
        y_train, y_test = y[:split_idx], y[split_idx:]
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)
        
        # –û–±—É—á–µ–Ω–∏–µ
        model = self.available_models[model_type]
        model.fit(X_train_scaled, y_train)
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        y_train_pred = model.predict(X_train_scaled)
        y_test_pred = model.predict(X_test_scaled)
        
        # –ú–µ—Ç—Ä–∏–∫–∏
        results = {
            'model_type': model_type,
            'train_metrics': {
                'mae': float(mean_absolute_error(y_train, y_train_pred)),
                'rmse': float(np.sqrt(mean_squared_error(y_train, y_train_pred))),
                'r2': float(r2_score(y_train, y_train_pred))
            },
            'test_metrics': {
                'mae': float(mean_absolute_error(y_test, y_test_pred)),
                'rmse': float(np.sqrt(mean_squared_error(y_test, y_test_pred))),
                'r2': float(r2_score(y_test, y_test_pred))
            },
            'train_size': len(X_train),
            'test_size': len(X_test),
            'features': list(X.columns),
            'trained_at': datetime.now().isoformat()
        }
        
        self.logger.info(f"–û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ. Test R¬≤: {results['test_metrics']['r2']:.4f}")
        
        return {
            'model': model,
            'scaler': scaler,
            'results': results
        }
    
    def save_model(self, ticker: str, model_data: Dict[str, Any], model_type: str) -> str:
        """
        –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏.
        
        Args:
            ticker (str): –¢–∏–∫–µ—Ä –∞–∫—Ü–∏–∏
            model_data (Dict[str, Any]): –î–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ (model, scaler, results)
            model_type (str): –¢–∏–ø –º–æ–¥–µ–ª–∏
        
        Returns:
            str: –ü—É—Ç—å –∫ —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
        """
        # –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Ç–∏–∫–µ—Ä–∞
        ticker_dir = self.models_dir / ticker
        ticker_dir.mkdir(parents=True, exist_ok=True)
        
        # –ò–º—è —Ñ–∞–π–ª–∞
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        model_file = ticker_dir / f"{ticker}_{model_type}_{timestamp}.pkl"
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
        with open(model_file, 'wb') as f:
            pickle.dump({
                'model': model_data['model'],
                'scaler': model_data['scaler'],
                'results': model_data['results']
            }, f)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –≤ JSON
        metrics_file = ticker_dir / f"{ticker}_{model_type}_{timestamp}_metrics.json"
        with open(metrics_file, 'w', encoding='utf-8') as f:
            json.dump(model_data['results'], f, indent=2, ensure_ascii=False)
        
        self.logger.info(f"–ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {model_file}")
        return str(model_file)
    
    def load_model(self, model_path: str) -> Dict[str, Any]:
        """
        –ó–∞–≥—Ä—É–∑–∫–∞ —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏.
        
        Args:
            model_path (str): –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –º–æ–¥–µ–ª–∏
        
        Returns:
            Dict[str, Any]: –î–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏
        """
        with open(model_path, 'rb') as f:
            model_data = pickle.load(f)
        
        self.logger.info(f"–ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {model_path}")
        return model_data
    
    def train_multiple_models(self, ticker: str, models: List[str] = None,
                            from_db: bool = True, target_horizon: int = 1) -> Dict[str, Dict]:
        """
        –û–±—É—á–µ–Ω–∏–µ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –º–æ–¥–µ–ª–µ–π –∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤.
        
        Args:
            ticker (str): –¢–∏–∫–µ—Ä –∞–∫—Ü–∏–∏
            models (List[str]): –°–ø–∏—Å–æ–∫ —Ç–∏–ø–æ–≤ –º–æ–¥–µ–ª–µ–π –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
            from_db (bool): –ó–∞–≥—Ä—É–∂–∞—Ç—å –¥–∞–Ω–Ω—ã–µ –∏–∑ –ë–î
            target_horizon (int): –ì–æ—Ä–∏–∑–æ–Ω—Ç –ø—Ä–æ–≥–Ω–æ–∑–∞
        
        Returns:
            Dict[str, Dict]: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏
        """
        if models is None:
            models = list(self.available_models.keys())
        
        self.logger.info(f"–û–±—É—á–µ–Ω–∏–µ {len(models)} –º–æ–¥–µ–ª–µ–π –¥–ª—è {ticker}")
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        quotes, indicators = self.load_training_data(ticker, from_db)
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        X, y = self.prepare_features(quotes, indicators, target_horizon)
        
        # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π
        results = {}
        for model_type in models:
            try:
                self.logger.info(f"–û–±—É—á–µ–Ω–∏–µ {model_type}...")
                model_data = self.train_model(X, y, model_type)
                
                # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
                model_path = self.save_model(ticker, model_data, model_type)
                
                results[model_type] = {
                    'metrics': model_data['results'],
                    'model_path': model_path
                }
            
            except Exception as e:
                self.logger.error(f"–û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è {model_type}: {str(e)}")
                results[model_type] = {'error': str(e)}
        
        # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π
        self._compare_models(results)
        
        return results
    
    def _compare_models(self, results: Dict[str, Dict]) -> None:
        """
        –í—ã–≤–æ–¥ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π.
        
        Args:
            results (Dict[str, Dict]): –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π
        
        Returns:
            None
        """
        self.logger.info("\n" + "="*80)
        self.logger.info("–°–†–ê–í–ù–ï–ù–ò–ï –ú–û–î–ï–õ–ï–ô")
        self.logger.info("="*80)
        
        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ Test R¬≤
        sorted_models = sorted(
            [(name, data) for name, data in results.items() if 'error' not in data],
            key=lambda x: x[1]['metrics']['test_metrics']['r2'],
            reverse=True
        )
        
        for name, data in sorted_models:
            metrics = data['metrics']['test_metrics']
            self.logger.info(f"\n{name.upper()}:")
            self.logger.info(f"  R¬≤: {metrics['r2']:.4f}")
            self.logger.info(f"  MAE: {metrics['mae']:.2f}")
            self.logger.info(f"  RMSE: {metrics['rmse']:.2f}")
        
        if sorted_models:
            best_model = sorted_models[0][0]
            self.logger.info(f"\nüèÜ –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å: {best_model.upper()}")
        
        self.logger.info("="*80 + "\n")
    
    def get_best_model(self, ticker: str) -> str:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ –ø—É—Ç–∏ –∫ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏ –¥–ª—è —Ç–∏–∫–µ—Ä–∞.
        
        Args:
            ticker (str): –¢–∏–∫–µ—Ä –∞–∫—Ü–∏–∏
        
        Returns:
            str: –ü—É—Ç—å –∫ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
        """
        ticker_dir = self.models_dir / ticker
        
        if not ticker_dir.exists():
            return ""
        
        # –ò—â–µ–º –≤—Å–µ –º–æ–¥–µ–ª–∏
        model_files = list(ticker_dir.glob("*_metrics.json"))
        
        if not model_files:
            return ""
        
        # –ù–∞—Ö–æ–¥–∏–º –º–æ–¥–µ–ª—å —Å –ª—É—á—à–∏–º R¬≤
        best_r2 = -float('inf')
        best_model_path = ""
        
        for metrics_file in model_files:
            with open(metrics_file, 'r') as f:
                metrics = json.load(f)
            
            r2 = metrics['test_metrics']['r2']
            if r2 > best_r2:
                best_r2 = r2
                # –ü–æ–ª—É—á–∞–µ–º –ø—É—Ç—å –∫ pkl —Ñ–∞–π–ª—É
                best_model_path = str(metrics_file).replace('_metrics.json', '.pkl')
        
        return best_model_path









